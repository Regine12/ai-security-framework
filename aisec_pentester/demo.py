#!/usr/bin/env python3
"""
AISec-Pentester Demo Module

Professional demonstration of AI security testing capabilities with enhanced reporting.
"""

import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path

# Add the parent directory to the path to fix imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.logger import get_logger
from core.reporting import SecurityReportGenerator

# Try to import production framework
try:
    from core.framework import AISec
except ImportError:
    AISec = None  # Will run in demo mode

# Initialize logger
logger = get_logger(__name__)

# Cross-platform console formatting
try:
    from rich.console import Console
    from rich.progress import Progress, SpinnerColumn, TextColumn
    console = Console()
except ImportError:
    # Fallback for systems without rich
    class SimpleConsole:
        def print(self, text, color='white', bold=False):
            if bold:
                print(f"[{color.upper()}] {text}")
            else:
                print(text)
        
        def input(self, prompt=""):
            return input(prompt)
    console = SimpleConsole()

def display_banner():
    """Display AISec-Pentester banner"""
    banner = """
╔═══════════════════════════════════════════════════════════════╗
║                     AISec-Pentester v2.0                     ║
║              Production AI Security Testing Framework          ║
║                                                               ║
║  • Adversarial Attack Testing    • Data Poisoning Detection  ║
║  • Model Extraction Analysis     • Privacy Leakage Scanning  ║
║  • MITRE ATLAS Integration       • OWASP LLM Compliance      ║
╚═══════════════════════════════════════════════════════════════╝
    """
    console.print(banner, 'blue', True)
    
    # Ethical warning
    console.print("", 'white')
    console.print("WARNING: ETHICAL USE ONLY", 'red', True)
    console.print("", 'white')
    console.print("This tool is designed for authorized security testing only.", 'yellow')
    console.print("• Only test systems you own or have explicit permission to test", 'white')
    console.print("• Follow responsible disclosure practices for any vulnerabilities found", 'white')
    console.print("• Comply with all applicable laws and regulations", 'white')
    console.print("• Unauthorized access to systems is illegal and unethical", 'white')
    console.print("", 'white')
    console.print("By proceeding, you acknowledge these responsibilities.", 'yellow')
    console.print("", 'white')

def demo_framework_initialization():
    """Demonstrate framework initialization"""
    console.print("Initializing AISec-Pentester Framework", 'cyan', True)
    
    if AISec is None:
        # Minimal demo mode
        console.print("Loading core framework... (demo mode)", 'yellow')
        time.sleep(1)
        console.print("[OK] Demo framework loaded", 'green')
        return None
    
    console.print("Loading core framework...", 'white')
    time.sleep(1)
    
    try:
        framework = AISec()
        console.print("[OK] Core framework loaded", 'green')
        
        console.print("Setting up test environment...", 'white')
        time.sleep(1)
        console.print("[OK] Test environment configured", 'green')
        
        console.print("Loading security modules...", 'white')
        time.sleep(1)
        console.print("[OK] Security modules ready", 'green')
        
        console.print("[OK] Framework initialization complete!", 'green')
        return framework
    
    except Exception as e:
        console.print(f"[ERROR] Framework initialization failed: {e}", 'red')
        console.print("Running in demo mode...", 'yellow')
        return None

def demo_comprehensive_assessment(framework):
    """Run a comprehensive security assessment demonstration"""
    console.print("", 'white')
    console.print("Running Comprehensive Security Assessment", 'cyan', True)
    
    console.print("Initializing assessment...", 'white')
    time.sleep(0.5)
    
    console.print("Loading test dataset (synthetic tabular data)...", 'yellow')
    time.sleep(1)
    
    console.print("Preparing test model (MLP classifier)...", 'yellow')
    time.sleep(1)
    
    console.print("Running adversarial attack testing...", 'blue')
    time.sleep(1)
    
    console.print("Running data poisoning detection...", 'blue')
    time.sleep(1)
    
    console.print("Running model extraction testing...", 'blue')
    time.sleep(1)
    
    console.print("Calculating security metrics...", 'green')
    time.sleep(1)
    
    console.print("Generating recommendations...", 'green')
    time.sleep(1)
    
    # Always run demo mode to avoid long downloads/computations
    console.print("Running lightweight demo assessment...", 'yellow')
    return _run_demo_assessment()

def _run_demo_assessment():
    """Run a lightweight demo assessment with simulated results"""
    # Simulate realistic assessment results without heavy computation
    return create_demo_results()

def create_demo_results():
    """Create realistic demo results"""
    return {
        'assessment_id': f"aisec_demo_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        'timestamp': datetime.now().isoformat(),
        'framework_version': '2.0.0',
        'target_info': {
            'model_type': 'TestMLP',
            'parameters': 4322,
            'device': 'cpu',
            'dataset': 'synthetic'
        },
        'adversarial_testing': {
            'test_type': 'adversarial_robustness',
            'attacks_tested': ['FGSM', 'PGD', 'C&W', 'DeepFool'],
            'attack_results': {
                'FGSM': {
                    'success_rate': 0.34,
                    'avg_perturbation_l2': 0.23,
                    'status': 'VULNERABLE'
                },
                'PGD': {
                    'success_rate': 0.67,
                    'avg_perturbation_l2': 0.31,
                    'status': 'VULNERABLE'
                },
                'C&W': {
                    'success_rate': 0.28,
                    'avg_perturbation_l2': 0.15,
                    'status': 'ROBUST'
                },
                'DeepFool': {
                    'success_rate': 0.45,
                    'avg_perturbation_l2': 0.19,
                    'status': 'VULNERABLE'
                }
            },
            'robustness_metrics': {
                'overall_robustness_score': 56.5,
                'vulnerability_count': 3,
                'robustness_level': 'MODERATELY_VULNERABLE'
            }
        },
        'poisoning_detection': {
            'test_type': 'data_poisoning_detection',
            'detection_results': {
                'Label Flipping': {
                    'detection_accuracy': 0.82,
                    'false_positive_rate': 0.05,
                    'status': 'DETECTED'
                },
                'Backdoor Injection': {
                    'detection_accuracy': 0.74,
                    'false_positive_rate': 0.08,
                    'status': 'DETECTED'
                },
                'Feature Manipulation': {
                    'detection_accuracy': 0.69,
                    'false_positive_rate': 0.06,
                    'status': 'MISSED'
                }
            },
            'overall_detection_metrics': {
                'average_detection_accuracy': 75.0,
                'detection_reliability': 'RELIABLE',
                'methods_detected': 2
            }
        },
        'extraction_testing': {
            'test_type': 'model_extraction',
            'extraction_results': {
                'Query-based': {
                    'extraction_success_rate': 0.42,
                    'model_fidelity': 0.87,
                    'queries_required': 15000,
                    'status': 'VULNERABLE'
                },
                'Gradient-based': {
                    'extraction_success_rate': 0.35,
                    'model_fidelity': 0.91,
                    'queries_required': 8500,
                    'status': 'PROTECTED'
                },
                'Side-channel': {
                    'extraction_success_rate': 0.18,
                    'model_fidelity': 0.62,
                    'queries_required': 2300,
                    'status': 'PROTECTED'
                }
            },
            'privacy_metrics': {
                'overall_privacy_score': 68.3,
                'extraction_vulnerability_count': 1,
                'privacy_level': 'MEDIUM_PRIVACY'
            }
        },
        'security_metrics': {
            'overall_risk_score': 58,
            'security_categories': {
                'adversarial_robustness': 56.5,
                'data_integrity': 75.0,
                'model_privacy': 68.3
            }
        },
        'recommendations': [
            {
                'priority': 'HIGH',
                'category': 'Adversarial Defense',
                'recommendation': 'Implement adversarial training with PGD and FGSM',
                'impact': 'HIGH',
                'effort': 'MEDIUM'
            },
            {
                'priority': 'MEDIUM',
                'category': 'Model Privacy',
                'recommendation': 'Implement differential privacy and query limiting',
                'impact': 'MEDIUM',
                'effort': 'HIGH'
            },
            {
                'priority': 'LOW',
                'category': 'Monitoring',
                'recommendation': 'Implement continuous security monitoring',
                'impact': 'MEDIUM',
                'effort': 'LOW'
            }
        ]
    }

def display_assessment_results(results):
    """Display comprehensive assessment results"""
    if not results or 'error' in results:
        console.print("No valid results to display", 'red')
        return
    
    console.print("", 'white')
    console.print("Security Assessment Results", 'cyan', True)
    console.print("", 'white')
    
    # Overview
    console.print("Assessment Overview:", 'blue', True)
    console.print(f"  Assessment ID: {results.get('assessment_id', 'N/A')}", 'white')
    console.print(f"  Framework Version: {results.get('framework_version', '2.0.0')}", 'white')
    console.print(f"  Timestamp: {results.get('timestamp', 'N/A')}", 'white')
    
    # Model info
    target_info = results.get('target_info', {})
    console.print(f"  Model Type: {target_info.get('model_type', 'N/A')}", 'white')
    console.print(f"  Model Parameters: {target_info.get('parameters', 'N/A')}", 'white')
    console.print(f"  Device: {target_info.get('device', 'N/A')}", 'white')
    console.print("", 'white')
    
    # Risk score
    security_metrics = results.get('security_metrics', {})
    risk_score = security_metrics.get('overall_risk_score', 50)
    risk_level = get_risk_level(risk_score)
    
    risk_color = 'red' if risk_score > 70 else 'yellow' if risk_score > 40 else 'green'
    console.print(f"Overall Risk Score: {risk_score}/100 ({risk_level})", risk_color, True)
    console.print("", 'white')
    
    # Detailed results
    display_detailed_results(results)
    
    # Recommendations
    display_recommendations(results)

def display_detailed_results(results):
    """Display detailed test results"""
    console.print("Detailed Test Results", 'cyan', True)
    console.print("", 'white')
    
    # Adversarial Testing Results
    if 'adversarial_testing' in results:
        adv_results = results['adversarial_testing']
        console.print("Adversarial Attack Testing:", 'yellow', True)
        
        attack_results = adv_results.get('attack_results', {})
        for attack, data in attack_results.items():
            success_rate = f"{data.get('success_rate', 0):.1%}"
            perturbation = f"{data.get('avg_perturbation_l2', 0):.3f}"
            status = data.get('status', 'UNKNOWN')
            status_color = 'red' if status == 'VULNERABLE' else 'green'
            
            console.print(f"  {attack}: {success_rate} success, {perturbation} perturbation", 'white')
            console.print(f"    Status: {status}", status_color)
        
        robustness = adv_results.get('robustness_metrics', {})
        robustness_score = robustness.get('overall_robustness_score', 0)
        console.print(f"  Overall Robustness Score: {robustness_score:.1f}/100", 'green')
        console.print("", 'white')
    
    # Data Poisoning Results
    if 'poisoning_detection' in results:
        poison_results = results['poisoning_detection']
        console.print("Data Poisoning Detection:", 'yellow', True)
        
        detection_results = poison_results.get('detection_results', {})
        for method, data in detection_results.items():
            detection_rate = f"{data.get('detection_accuracy', 0):.1%}"
            false_positive = f"{data.get('false_positive_rate', 0):.1%}"
            status = data.get('status', 'UNKNOWN')
            status_color = 'green' if status == 'DETECTED' else 'red'
            
            console.print(f"  {method}: {detection_rate} detection, {false_positive} false positive", 'white')
            console.print(f"    Status: {status}", status_color)
        
        overall_metrics = poison_results.get('overall_detection_metrics', {})
        avg_detection = overall_metrics.get('average_detection_accuracy', 0)
        console.print(f"  Average Detection Accuracy: {avg_detection:.1f}%", 'green')
        console.print("", 'white')
    
    # Model Extraction Results
    if 'extraction_testing' in results:
        extract_results = results['extraction_testing']
        console.print("Model Extraction Testing:", 'yellow', True)
        
        extraction_results = extract_results.get('extraction_results', {})
        for method, data in extraction_results.items():
            success_rate = f"{data.get('extraction_success_rate', 0):.1%}"
            fidelity = f"{data.get('model_fidelity', 0):.1%}"
            queries = data.get('queries_required', 0)
            status = data.get('status', 'UNKNOWN')
            status_color = 'red' if status == 'VULNERABLE' else 'green'
            
            console.print(f"  {method}: {success_rate} success, {fidelity} fidelity, {queries} queries", 'white')
            console.print(f"    Status: {status}", status_color)
        
        privacy_metrics = extract_results.get('privacy_metrics', {})
        privacy_score = privacy_metrics.get('overall_privacy_score', 0)
        console.print(f"  Overall Privacy Score: {privacy_score:.1f}/100", 'green')
        console.print("", 'white')

def display_recommendations(results):
    """Display security recommendations"""
    recommendations = results.get('recommendations', [])
    if not recommendations:
        return
    
    console.print("Security Recommendations", 'cyan', True)
    console.print("", 'white')
    
    # Group recommendations by priority
    critical = [r for r in recommendations if r.get('priority') == 'CRITICAL']
    high = [r for r in recommendations if r.get('priority') == 'HIGH']
    medium = [r for r in recommendations if r.get('priority') == 'MEDIUM']
    low = [r for r in recommendations if r.get('priority') == 'LOW']
    
    if critical:
        console.print("CRITICAL PRIORITY:", 'red', True)
        for rec in critical:
            console.print(f"  • {rec.get('recommendation', 'N/A')}", 'white')
        console.print("", 'white')
    
    if high:
        console.print("HIGH PRIORITY:", 'yellow', True)
        for rec in high:
            console.print(f"  • {rec.get('recommendation', 'N/A')}", 'white')
        console.print("", 'white')
    
    if medium:
        console.print("MEDIUM PRIORITY:", 'blue', True)
        for rec in medium:
            console.print(f"  • {rec.get('recommendation', 'N/A')}", 'white')
        console.print("", 'white')
    
    if low:
        console.print("LOW PRIORITY:", 'green', True)
        for rec in low:
            console.print(f"  • {rec.get('recommendation', 'N/A')}", 'white')

def get_risk_level(risk_score):
    """Get risk level based on score"""
    if risk_score >= 80:
        return "CRITICAL"
    elif risk_score >= 60:
        return "HIGH"
    elif risk_score >= 40:
        return "MEDIUM"
    elif risk_score >= 20:
        return "LOW"
    else:
        return "VERY LOW"

def demo_individual_modules(framework):
    """Demonstrate individual testing modules - lightweight version"""
    console.print("", 'white')
    console.print("Individual Module Testing (Quick Demo)", 'cyan', True)
    console.print("", 'white')
    
    modules = [
        ("Adversarial Testing", "Testing model robustness against adversarial attacks"),
        ("Poisoning Detection", "Scanning for data poisoning in training datasets"), 
        ("Extraction Testing", "Evaluating model privacy and extraction vulnerabilities")
    ]
    
    for module_name, description in modules:
        console.print(f"Testing {module_name}: {description}", 'yellow')
        
        # Quick simulation without heavy computation
        time.sleep(0.2)
        console.print("  ✓ Module initialized", 'white')
        time.sleep(0.2)
        console.print("  ✓ Test completed", 'white')
        time.sleep(0.2)
        console.print(f"  [OK] {module_name} - Demo successful", 'green')
        console.print("", 'white')

def save_results_demo(results):
    """Enhanced results saving with professional reporting"""
    if not results or 'error' in results:
        return
    
    console.print("Saving Assessment Results", 'cyan', True)
    
    # Create output directory
    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)
    
    # Initialize report generator
    report_gen = SecurityReportGenerator()
    
    # Add demo findings
    report_gen.add_finding(
        severity="Low",
        title="Demo Assessment Completed",
        description="This is a demonstration assessment showing the framework's capabilities. No actual vulnerabilities were tested in demo mode.",
        category="Framework Demo",
        cvss_score=1.0,
        affected_components=["Demo System"],
        recommendations=["Consider running a full production assessment for comprehensive security analysis"]
    )
    
    # Add demo recommendations
    report_gen.add_recommendation(
        priority="Medium",
        title="Upgrade to Production Framework",
        description="Consider implementing the full AISec-Pentester framework for comprehensive AI security testing in production environments.",
        effort="Medium",
        impact="High"
    )
    
    report_gen.add_recommendation(
        priority="Low", 
        title="Regular Security Assessments",
        description="Implement regular AI security assessments as part of your security lifecycle.",
        effort="Low",
        impact="Medium"
    )
    
    # Generate reports
    assessment_id = results.get('assessment_id', 'demo')
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    try:
        # Save JSON report (legacy compatibility)
        json_file = output_dir / f"security_assessment_{assessment_id}.json"
        with open(json_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        # Generate professional HTML report
        html_file = output_dir / f"security_report_{assessment_id}_{timestamp}.html"
        report_gen.generate_html_report(str(html_file), results)
        
        # Generate summary report for checklist export
        summary_file = output_dir / f"assessment_summary_{assessment_id}_{timestamp}.json"
        summary_data = report_gen.generate_summary_report(results)
        with open(summary_file, 'w') as f:
            json.dump(summary_data, f, indent=2, default=str)
        
        console.print(f"[OK] Professional HTML report: {html_file}", 'green')
        console.print(f"[OK] Assessment summary: {summary_file}", 'green') 
        console.print(f"[OK] Raw data (JSON): {json_file}", 'green')
        
        # Show file sizes
        html_size = html_file.stat().st_size
        json_size = json_file.stat().st_size
        console.print(f"   HTML Report: {html_size:,} bytes", 'white')
        console.print(f"   JSON Data: {json_size:,} bytes", 'white')
        
        console.print("\nReport Features:", 'cyan')
        console.print("  • Professional HTML formatting", 'white')
        console.print("  • Executive summary with statistics", 'white') 
        console.print("  • Detailed findings with severity ratings", 'white')
        console.print("  • Security recommendations with priorities", 'white')
        console.print("  • Print-friendly layout", 'white')
        
    except Exception as e:
        console.print(f"[ERROR] Failed to save results: {e}", 'red')

def show_help():
    """Show help information"""
    help_text = """
AISec-Pentester Demo - AI Security Testing Framework

USAGE:
    python3 demo.py [OPTIONS]

OPTIONS:
    --help, -h          Show this help message
    --quick             Run quick demo (individual modules only)
    --full              Run full comprehensive assessment
    --no-color          Disable colored output

EXAMPLES:
    python3 demo.py                 # Interactive demo
    python3 demo.py --quick         # Quick module demo
    python3 demo.py --full          # Full assessment demo

For production use:
    pip install aisec-pentester     # Install full framework
    aisec assess --help             # CLI help
    aisec assess --comprehensive    # Run full assessment
    """
    console.print(help_text, 'white')

def main():
    """Main demo function"""
    # Check for help flag
    if '--help' in sys.argv or '-h' in sys.argv:
        show_help()
        return
    
    # Skip ethical prompt for automated testing
    if '--quick' in sys.argv:
        console.print("Running in quick demo mode...", 'yellow')
    else:
        try:
            # Display banner and warnings
            display_banner()
            
            # Ask for user consent
            consent = console.input("Do you acknowledge the ethical use requirements? (yes/no): ")
            if consent.lower() not in ['yes', 'y']:
                console.print("Demo aborted. Ethical acknowledgment required.", 'red')
                return
            
            console.print("Ethical acknowledgment confirmed. Proceeding with demo...", 'green')
        except (EOFError, KeyboardInterrupt):
            console.print("Demo aborted by user.", 'red')
            return
    
    try:
        # Initialize framework
        framework = demo_framework_initialization()
        
        # Check for quick demo flag
        if '--quick' in sys.argv:
            demo_individual_modules(framework)
            console.print("Quick Demo Complete!", 'green', True)
            return
        
        # Check for full demo flag
        if '--full' in sys.argv:
            results = demo_comprehensive_assessment(framework)
            if results:
                display_assessment_results(results)
                save_results_demo(results)
            console.print("Full Demo Complete!", 'green', True)
            return
        
        # Interactive demo
        console.print("", 'white')
        console.print("Select Demo Type:", 'cyan', True)
        console.print("1. Comprehensive Assessment (Full security evaluation)", 'green')
        console.print("2. Individual Modules (Test specific components)", 'yellow')
        console.print("3. Both (Complete demonstration)", 'blue')
        
        choice = console.input("Enter your choice (1-3): ")
        
        results = None
        
        if choice in ['1', '3']:
            # Run comprehensive assessment
            results = demo_comprehensive_assessment(framework)
            if results:
                display_assessment_results(results)
                save_results_demo(results)
        
        if choice in ['2', '3']:
            # Demo individual modules
            demo_individual_modules(framework)
        
        # Final summary
        console.print("", 'white')
        console.print("AISec-Pentester Demo Complete!", 'green', True)
        console.print("", 'white')
        console.print("Next Steps:", 'cyan')
        console.print("• Install full version: pip install aisec-pentester", 'white')
        console.print("• View documentation: https://github.com/Regine12/AISec-pentester", 'white')
        console.print("• Run on your models: aisec assess --help", 'white')
        console.print("• Configure settings: ~/.aisec/config.yaml", 'white')
        
    except KeyboardInterrupt:
        console.print("", 'white')
        console.print("Demo interrupted by user. Goodbye!", 'yellow')
    except Exception as e:
        console.print(f"Demo error: {e}", 'red')
        logger.error(f"Demo failed: {e}")


if __name__ == "__main__":
    main()
